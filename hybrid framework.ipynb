{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "594bdda4",
      "metadata": {
        "id": "594bdda4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d33e2372",
      "metadata": {
        "id": "d33e2372"
      },
      "outputs": [],
      "source": [
        "path = '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#connect to self drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMTV57FLHlkW",
        "outputId": "6fe8049a-9f8f-4167-a524-7ad6d190b284"
      },
      "id": "PMTV57FLHlkW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "0pd1ZIVJrn3_",
        "outputId": "61342e95-2876-49b1-d688-59ef640aed7e"
      },
      "id": "0pd1ZIVJrn3_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1SkNv4dsaMP",
        "outputId": "f3439832-8317-4397-be30-c4ac55eb8346"
      },
      "id": "U1SkNv4dsaMP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: gensim\n",
            "Version: 3.6.0\n",
            "Summary: Python framework for fast Vector Space Modelling\n",
            "Home-page: http://radimrehurek.com/gensim\n",
            "Author: Radim Rehurek\n",
            "Author-email: me@radimrehurek.com\n",
            "License: LGPLv2.1\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: smart-open, numpy, six, scipy\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dfb7ac0",
      "metadata": {
        "id": "8dfb7ac0"
      },
      "source": [
        "# 1. online_shopping_10_cats.csv\n",
        "data comes from :https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/online_shopping_10_cats/intro.ipynb\n",
        "load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d65d9955",
      "metadata": {
        "id": "d65d9955"
      },
      "outputs": [],
      "source": [
        "\n",
        "pd_all = pd.read_csv(\"/online_shopping_10_cats.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93036715",
      "metadata": {
        "id": "93036715"
      },
      "source": [
        "# \n",
        "Extraction of data from specified categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "625ab4a4",
      "metadata": {
        "id": "625ab4a4"
      },
      "outputs": [],
      "source": [
        "target_cats = ['蒙牛', '水果'] \n",
        "\n",
        "pd_data = pd_all[pd_all.cat.isin(target_cats)]\n",
        "\n",
        "\n",
        "\n",
        "pd_data.sample(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f585c991",
      "metadata": {
        "id": "f585c991"
      },
      "outputs": [],
      "source": [
        "pd_data = pd_data.dropna(subset=[\"review\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f0dd2c5",
      "metadata": {
        "id": "4f0dd2c5"
      },
      "outputs": [],
      "source": [
        "#save the balanced corpus\n",
        "pd_data.to_csv(path+'fresh_food.csv',header=True,index=None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd_use = pd.read_csv(\"/content/fresh_food.csv\",index_col=None)\n",
        "    \n",
        "pos = pd_use[pd_use.label==1]\n",
        "neg = pd_use[pd_use.label==0]\n",
        "pos.to_csv(path+'pos.csv',header=True,index=None)"
      ],
      "metadata": {
        "id": "RI9q9OtgZdDF"
      },
      "id": "RI9q9OtgZdDF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neg.to_csv(path+'neg.csv',header=True,index=None)"
      ],
      "metadata": {
        "id": "Dvz2vakgZvm8"
      },
      "id": "Dvz2vakgZvm8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GULz0TslU1na"
      },
      "id": "GULz0TslU1na"
    },
    {
      "cell_type": "markdown",
      "id": "7c8698bc",
      "metadata": {
        "id": "7c8698bc"
      },
      "source": [
        "# 2. word2vec Vectors training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c5164d1",
      "metadata": {
        "id": "5c5164d1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import jieba\n",
        "import multiprocessing\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.decomposition import IncrementalPCA as PCA   \n",
        "from sklearn.manifold import TSNE                  \n",
        "import plotly.graph_objs as go\n",
        "import seaborn as sns\n",
        "\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "   \n",
        "    jieba.load_userdict(\"/content/drive/MyDrive/SogouLabDic.txt\")\n",
        "    jieba.load_userdict(\"/content/drive/MyDrive/dict_baidu_utf8.txt\")\n",
        "    jieba.load_userdict(\"/content/drive/MyDrive/dict_pangu.txt\")\n",
        "    jieba.load_userdict(\"/content/drive/MyDrive/dict_sougou_utf8.txt\")\n",
        "    jieba.load_userdict(\"/content/drive/MyDrive/dict_tencent_utf8.txt\")\n",
        "  "
      ],
      "metadata": {
        "id": "hCZJF-VYkwQ3"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hCZJF-VYkwQ3"
    },
    {
      "cell_type": "code",
      "source": [
        "#splits + remove stopwords\n",
        "def cut_words(data):\n",
        "\n",
        "    stopwords = {}.fromkeys([ line.rstrip() for line in open('/content/drive/MyDrive/cn_stopwords.txt','r',encoding='utf8') ])\n",
        "    print(len(data))\n",
        "    Result = []\n",
        "    for item in data :\n",
        "        #print(item)\n",
        "        #print(type(item))\n",
        "        seg = jieba.cut(item)\n",
        "        res = [i for i in seg if i not in stopwords]\n",
        "        Result.append(res)\n",
        "\n",
        "    return Result"
      ],
      "metadata": {
        "id": "sOCT6EPLU-Tf"
      },
      "execution_count": null,
      "outputs": [],
      "id": "sOCT6EPLU-Tf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42fe6e05",
      "metadata": {
        "id": "42fe6e05"
      },
      "outputs": [],
      "source": [
        "def loadfile():\n",
        "    pd_use = pd.read_csv(\"/content/drive/MyDrive/fresh_food.csv\",index_col=None)\n",
        "    \n",
        "    pos = pd_use[pd_use.label==1]\n",
        "    neg = pd_use[pd_use.label==0]\n",
        "    #print(pos)\n",
        "    \n",
        "    #neg = pd.read_csv('./data/neg.csv', header=None, index_col=None)\n",
        "    #pos = pd.read_csv('./data/pos.csv', header=None, index_col=None)\n",
        "\n",
        "    combined = np.concatenate((pos[\"review\"], neg[\"review\"]))\n",
        "    #combined = pd_use[\"review\"]\n",
        "    print(type(combined))\n",
        "    y = np.concatenate((np.ones(len(pos), dtype=int), np.zeros(len(neg), dtype=int)))\n",
        "    \n",
        "\n",
        "    return combined, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0e22512",
      "metadata": {
        "id": "d0e22512"
      },
      "outputs": [],
      "source": [
        "\n",
        "def tokenizer(data):\n",
        "    text = [jieba.lcut(document.replace('\\n', '')) for document in data]\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a569d5cf",
      "metadata": {
        "id": "a569d5cf"
      },
      "outputs": [],
      "source": [
        "# Adapted from Cyber Inovation Lab, github.com/das-lab/TAP/blob/master/tap.py\n",
        "cpu_count = multiprocessing.cpu_count()\n",
        "vocab_dim = 100\n",
        "n_iterations = 1\n",
        "n_exposures = 10  # All words with frequency above 10\n",
        "window_size = 7\n",
        "\n",
        "\n",
        "def word2vec_train(combined):\n",
        "    model = Word2Vec(size=vocab_dim,\n",
        "                     min_count=n_exposures,\n",
        "                     window=window_size,\n",
        "                     workers=cpu_count,\n",
        "                     iter=n_iterations)\n",
        "    model.build_vocab(combined)  # input: list\n",
        "    model.train(combined, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "    model.save('/content/drive/MyDrive/model/Word2vec_model_without.pkl')\n",
        "   \n",
        "\n",
        "\n",
        "# Word vectors summed to average\n",
        "def fea_sentence(list_w):\n",
        "    n0 = np.array([0. for i in range(vocab_dim)], dtype=np.float32)\n",
        "    for i in list_w:\n",
        "        n0 += i\n",
        "    fe = n0 / len(list_w)\n",
        "    fe = fe.tolist()\n",
        "    return fe\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa6d8b51",
      "metadata": {
        "id": "aa6d8b51"
      },
      "outputs": [],
      "source": [
        "# train model and save\n",
        "print('load data')\n",
        "combined, y = loadfile()\n",
        "print(len(combined))\n",
        "print('data preparation')\n",
        "combined = tokenizer(combined)\n",
        "#combined = cut_words(combined)\n",
        "print('train word2vec')\n",
        "word2vec_train(combined)\n",
        "## If the required word2vec has already been trained, the above lines of code can be commented out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('load word2vec model')\n",
        "w2v= Word2Vec.load('/content/drive/MyDrive/model/Word2vec_model.pkl')\n"
      ],
      "metadata": {
        "id": "Lh9Hb3-LKS7k"
      },
      "id": "Lh9Hb3-LKS7k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ec2321c",
      "metadata": {
        "id": "4ec2321c"
      },
      "outputs": [],
      "source": [
        "print('load word2vec model')\n",
        "w2v_without= Word2Vec.load('/content/drive/MyDrive/model/Word2vec_model_without.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73f093c2",
      "metadata": {
        "id": "73f093c2"
      },
      "source": [
        "# networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "708f0ad5",
      "metadata": {
        "id": "708f0ad5"
      },
      "outputs": [],
      "source": [
        "pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1675f039",
      "metadata": {
        "id": "1675f039"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show keras"
      ],
      "metadata": {
        "id": "YjOKvlf4lYCI"
      },
      "id": "YjOKvlf4lYCI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip show tensorflow"
      ],
      "metadata": {
        "id": "OspWXNdglfMJ"
      },
      "id": "OspWXNdglfMJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf54def",
      "metadata": {
        "id": "bdf54def"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "import csv\n",
        "import random\n",
        "\n",
        "import jieba\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import multiprocessing\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import keras\n",
        "from keras import Input, Model\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import load_model\n",
        "from keras.layers import Conv1D, MaxPooling1D, concatenate\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.core import Dense, Dropout, Flatten\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Adapted from danan0755, GitHub.com/xyzgithub.com/danan0755/Bert_Classifier/blob/master/sentiment_predict.py\n",
        "# Adapted from Cyber Inovation Lab, github.com/das-lab/TAP/blob/master/tap.py\n",
        "cpu_count = multiprocessing.cpu_count()\n",
        "vocab_dim = 100\n",
        "n_iterations = 1\n",
        "n_exposures = 10 # All words with frequency above 10\n",
        "window_size = 7\n",
        "n_epoch = 30\n",
        "maxlen = 30\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "\n",
        "def create_dictionaries(model=None, combined=None):\n",
        "  #w2indx - A dictionary that maps all words in a dataset into serial numbers\n",
        "  #w2vec - A dictionary that maps all words in a dataset into vectors\n",
        "\n",
        "    if (combined is not None) and (model is not None):\n",
        "        gensim_dict = Dictionary()\n",
        "        gensim_dict.doc2bow(model.wv.vocab.keys(),\n",
        "                            allow_update=True)\n",
        "        #  freqxiao10->0 所以k+1\n",
        "        w2indx = {v: k + 1 for k, v in gensim_dict.items()}  #Index of all words with frequency over 10, (k->v) => (v->k)\n",
        "\n",
        "\n",
        "\n",
        "        w2vec = {word: model[word] for word in w2indx.keys()}  # # The word vector of all words with frequency over 10, (word->model(word)) \n",
        "\n",
        "        def parse_dataset1(combined):  \n",
        "            data = []\n",
        "            for sentence in combined:\n",
        "                new_txt = []\n",
        "                for word in sentence:\n",
        "                    try:\n",
        "                        new_txt.append(w2indx[word])\n",
        "                    except:\n",
        "                        new_txt.append(0)  # freqxiao10->0\n",
        "                data.append(new_txt)\n",
        "            return data  # word=>index\n",
        "\n",
        "        combined = parse_dataset1(combined)  # [[1,2,3...],[]]\n",
        "\n",
        " # The length of each sentence vector must be uniform, but the sentence length is not of a uniform form,\n",
        " # and a maxlen is set as the maximum sentence length value\n",
        " #The index corresponding to the word contained in each sentence, so sentences containing words with a frequency of less than 10 have an index of 0\n",
        "        combined = sequence.pad_sequences(combined, maxlen=maxlen) \n",
        "        return w2indx, w2vec, combined\n",
        "    else:\n",
        "        print('No data provided...')\n",
        "\n",
        "\n",
        "\n",
        "def use_word2vec_model(path,combined):\n",
        "    model= Word2Vec.load(path)\n",
        "    index_dict, word_vectors, combined = create_dictionaries(model=model, combined=combined)\n",
        "    return index_dict, word_vectors, combined\n",
        "\n",
        "\n",
        "def get_data_fortextcnn(index_dict, word_vectors, combined, y):\n",
        "    n_symbols = len(index_dict) + 1  \n",
        "    embedding_weights = np.zeros((n_symbols, vocab_dim))  \n",
        "    for word, index in index_dict.items(): \n",
        "        embedding_weights[index, :] = word_vectors[word]\n",
        "    x_train, x_test, y_train, y_test = train_test_split(combined, y, test_size=0.2, random_state=5)\n",
        "    y_train = keras.utils.np_utils.to_categorical(y_train, num_classes=2)  # convert to one-hot 表示  [len(y),2]\n",
        "    y_test = keras.utils.np_utils.to_categorical(y_test, num_classes=2)\n",
        "\n",
        "    return n_symbols, embedding_weights, x_train, y_train, x_test, y_test\n",
        "\n",
        "\n",
        "##define network\n",
        "def train_cnn(n_symbols, embedding_weights, x_train, y_train):\n",
        "    main_input = Input(shape=(maxlen,), dtype='float64')\n",
        "    embedder = Embedding(output_dim=vocab_dim,\n",
        "                         input_dim=n_symbols,\n",
        "                         input_length=maxlen,\n",
        "                         weights=[embedding_weights])\n",
        "    embed = embedder(main_input)\n",
        "    cnn1 = Conv1D(64, 3, padding='same', strides=1, activation='relu')(embed)\n",
        "    cnn1 = MaxPooling1D(pool_size=19)(cnn1)\n",
        "    cnn2 = Conv1D(64, 4, padding='same', strides=1, activation='relu')(embed)\n",
        "    cnn2 = MaxPooling1D(pool_size=18)(cnn2)\n",
        "    cnn3 = Conv1D(64, 5, padding='same', strides=1, activation='relu')(embed)\n",
        "    cnn3 = MaxPooling1D(pool_size=17)(cnn3)\n",
        "    cnn = concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
        "    flat = Flatten()(cnn)\n",
        "    drop = Dropout(0.5)(flat)\n",
        "    main_output = Dense(2, activation='softmax')(drop)\n",
        "    model = Model(inputs=main_input, outputs=main_output)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epoch)\n",
        "\n",
        "    model.save('/content/drive/MyDrive/model/textcnn_w2v.h5')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f55711b",
      "metadata": {
        "id": "6f55711b"
      },
      "outputs": [],
      "source": [
        "\n",
        "print('load word2vec ')\n",
        "index_dict, word_vectors, combined = use_word2vec_model('/content/drive/MyDrive/model/Word2vec_model.pkl',combined)\n",
        "\n",
        "print(' convert data format')\n",
        "n_symbols, embedding_weights, x_train, y_train, x_test, y_test = get_data_fortextcnn(index_dict, word_vectors, combined,y)\n",
        "print(\"features&labels size:\")\n",
        "print(x_train.shape, y_train.shape)\n",
        "\n",
        "print('train textcnn')\n",
        "train_cnn(n_symbols, embedding_weights, x_train, y_train)\n",
        "\n",
        "print('load textcnn model')\n",
        "model = load_model('/content/drive/MyDrive/model/textcnn_w2v.h5')\n",
        "y_pred = model.predict(x_test)\n",
        "for i in range(len(y_pred)):\n",
        "    max_value = max(y_pred[i])\n",
        "    for j in range(len(y_pred[i])):\n",
        "        if max_value == y_pred[i][j]:\n",
        "            y_pred[i][j] = 1\n",
        "        else:\n",
        "            y_pred[i][j] = 0\n",
        "# target_names = ['neg', 'pos']\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bi-LSTM"
      ],
      "metadata": {
        "id": "aqa3m5N0Z51k"
      },
      "id": "aqa3m5N0Z51k"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Bidirectional, Activation\n"
      ],
      "metadata": {
        "id": "GsM318CNZ_SA"
      },
      "id": "GsM318CNZ_SA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def train_bilstm(n_symbols, embedding_weights, x_train, y_train):\n",
        "    print('Defining a Simple Keras Model...')\n",
        "    model = Sequential()  # or Graph or whatever\n",
        "    model.add(Embedding(output_dim=vocab_dim,\n",
        "                        input_dim=n_symbols,\n",
        "                        mask_zero=True,\n",
        "                        weights=[embedding_weights],\n",
        "                        input_length=maxlen))  # Adding Input Length\n",
        "    # model.add(LSTM(output_dim=50, activation='tanh'))\n",
        "    model.add(Bidirectional(LSTM(64, activation='tanh')))\n",
        "   # model.add(LSTM(64, activation='tanh'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2, activation='softmax'))  # Dense=>Fully connected layer, output dimension = 2\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epoch)\n",
        "\n",
        "    model.save('/content/drive/MyDrive/model/bilstm_w2v.h5')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eVFZSVFZaPB4"
      },
      "id": "eVFZSVFZaPB4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "    print('train bilstm model')\n",
        "    train_bilstm(n_symbols, embedding_weights, x_train, y_train)\n",
        "\n",
        "    print('load bilstm model')\n",
        "    model = load_model('/content/drive/MyDrive/model/bilstm_w2v.h5')\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    for i in range(len(y_pred)):\n",
        "        max_value = max(y_pred[i])\n",
        "        for j in range(len(y_pred[i])):\n",
        "            if max_value == y_pred[i][j]:\n",
        "                y_pred[i][j] = 1\n",
        "            else:\n",
        "                y_pred[i][j] = 0\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "vMS4GTz3auPB"
      },
      "id": "vMS4GTz3auPB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cnnbilstm"
      ],
      "metadata": {
        "id": "XJCho1dyb1Sh"
      },
      "id": "XJCho1dyb1Sh"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.layers import Conv1D, MaxPooling1D, concatenate\n",
        "from keras.layers import Bidirectional\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "def train_cnn_bilstm(n_symbols, embedding_weights, x_train, y_train):\n",
        "    #  Model structure: word embedding - convolutional pooling - splicing - BiLSTM - fully connected - dropout - fully connected\n",
        "    main_input = Input(shape=(maxlen,), dtype='float64')\n",
        "    embedder = Embedding(output_dim=vocab_dim,\n",
        "                         input_dim=n_symbols,\n",
        "                         input_length=maxlen,\n",
        "                         weights=[embedding_weights])\n",
        "    embed = embedder(main_input)\n",
        "    cnn = Conv1D(64, 3, padding='same', strides=1, activation='relu')(embed)\n",
        "    bilstm = Bidirectional(LSTM(64, dropout=0.5, activation='tanh', return_sequences=True))(cnn)\n",
        "    flat = Flatten()(bilstm)\n",
        "    main_output = Dense(2, activation='softmax')(flat)\n",
        "    model = Model(inputs=main_input, outputs=main_output)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epoch)\n",
        "\n",
        "    model.save('/content/drive/MyDrive/model/cnnbilstm.h5')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "btkSHUv3cBbE"
      },
      "id": "btkSHUv3cBbE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    print('train cnn_bilstm')\n",
        "    train_cnn_bilstm(n_symbols, embedding_weights, x_train, y_train)\n",
        "\n",
        "    print('load cnn_bilstm')\n",
        "    model = load_model('/content/drive/MyDrive/model/cnnbilstm.h5')\n",
        "    y_pred = model.predict(x_test)\n",
        "    for i in range(len(y_pred)):\n",
        "        max_value = max(y_pred[i])\n",
        "        for j in range(len(y_pred[i])):\n",
        "            if max_value == y_pred[i][j]:\n",
        "                y_pred[i][j] = 1\n",
        "            else:\n",
        "                y_pred[i][j] = 0\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "VI3JuKosenyZ"
      },
      "id": "VI3JuKosenyZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Topa5QoAQ-2"
      },
      "source": [
        "# 4.topic extraction\n",
        "\n",
        "tf-idf"
      ],
      "id": "1Topa5QoAQ-2"
    },
    {
      "cell_type": "code",
      "source": [
        "pip show jieba"
      ],
      "metadata": {
        "id": "pRwk9gu3rpNz"
      },
      "id": "pRwk9gu3rpNz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeTH6xaj85zC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import jieba\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "import jieba.analyse\n",
        "\n",
        "from jieba import analyse\n"
      ],
      "id": "WeTH6xaj85zC"
    },
    {
      "cell_type": "code",
      "source": [
        "#Get word vector word lists\n",
        "def get_model_voca(model):\n",
        "   words = model.wv.index2word\n",
        "   #words = model.wv.index_to_key\n",
        "   vocablist = []\n",
        "   for word in words:  \n",
        "     vocablist.append(word)\n",
        "   return vocablist"
      ],
      "metadata": {
        "id": "MUeRyFfeis4j"
      },
      "id": "MUeRyFfeis4j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get similar words\n",
        "def get_sims_words(word,vocablist,model):\n",
        "\n",
        "  if word  in vocablist:\n",
        "    sims = model.wv.most_similar(word, topn=10)  # get other similar words\n",
        "    simslist = []\n",
        "\n",
        "    i=0\n",
        "    for i in range(0,len(sims)):\n",
        "      voca = sims[i][0]\n",
        "      simslist.append(voca)\n",
        "      i=i+1\n",
        "  \n",
        "  else:\n",
        "    simslist = []\n",
        "  \n",
        "  return simslist"
      ],
      "metadata": {
        "id": "h-pPD1d8WsQv"
      },
      "id": "h-pPD1d8WsQv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get topic\n",
        "def get_attributes(keywordsdic):\n",
        "   empty=[]\n",
        "   for k in keywordsdic.keys():\n",
        "     empty.append(k)\n",
        "   return empty\n"
      ],
      "metadata": {
        "id": "VcrR_PQlbbPp"
      },
      "id": "VcrR_PQlbbPp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUQSWphZAQ-3"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/fresh_food.csv', header=0, encoding='utf-8-sig')['review']\n",
        "#remove stopwords\n",
        "#tfidf = analyse.extract_tags\n",
        "jieba.analyse.set_stop_words('/content/drive/MyDrive/cn_stopwords.txt')\n",
        "for line in data:\n",
        "    #Exclusion of empty lines\n",
        "    if line.strip() == '':\n",
        "        \n",
        "        continue\n",
        "\n",
        "    text = line\n",
        "        #tfidf starts with statistical information about words only and does not sufficiently consider the semantic information between words\n",
        "       \n",
        "    keywords = tfidf(text,\n",
        "                         allowPOS=('ns', 'nr', 'nt', 'nz', 'nl', 'n', 'vn', 'vd', 'vg', 'v', 'vf', 'a', 'an', 'i'))\n",
        "    result = []\n",
        "\n",
        "    for keyword in keywords:\n",
        "        result.append(keyword)\n",
        "    fo = open('/content/drive/MyDrive/keywords_withoutstop.txt', \"a+\", encoding='utf-8')  # generate keywords for every review\n",
        "    for j in result:\n",
        "        fo.write(j)\n",
        "        fo.write(' ')\n",
        "    fo.write('\\n')\n",
        "    fo.close()\n",
        "\n",
        "kwords = ''\n",
        "f = open('/content/drive/MyDrive/keywords_withoutstop.txt', 'r', encoding='utf-8')\n",
        "\n",
        "for i in f:\n",
        "    kwords += f.read()\n",
        "\n",
        "\n",
        "    # TextRank\n",
        "result = jieba.analyse.textrank(kwords, topK=50, withWeight=True)\n",
        "\n",
        "keywords = dict()\n",
        "for i in result:\n",
        "    keywords[i[0]] = i[1]\n",
        "print(keywords)\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "LUQSWphZAQ-3"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "filename = open('/content/drive/MyDrive/whole_corpus_withstopwords.txt','w')#dict to txt\n",
        "for k,v in keywords.items():\n",
        "    filename.write(k+':'+str(v))\n",
        "    filename.write('\\n')\n",
        "filename.close()"
      ],
      "metadata": {
        "id": "BZ8PdoCP9lVP"
      },
      "id": "BZ8PdoCP9lVP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "vectorwords = get_model_voca(w2v)\n",
        "#change word2vec version\n",
        "#vectorwords = get_model_voca(w2v_without)\n",
        "attributes = get_attributes(keywords)\n",
        "\n",
        "attri_result= dict()\n",
        "topics =[]\n",
        "\n",
        "for attribute in attributes:\n",
        "  samiliarity  = get_sims_words(attribute,vectorwords,w2v)\n",
        "  #topics for coherance\n",
        "  topics.append(samiliarity)\n",
        "  attri_result[attribute] = samiliarity\n",
        "\n",
        "\n",
        "filename = open('/content/drive/MyDrive/attributs_withstop_w2v.txt','w')#dict to txt\n",
        "for k,v in attri_result.items():\n",
        "    filename.write(k+':'+str(v))\n",
        "    filename.write('\\n')\n",
        "filename.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ld7j4M6vq7Nf"
      },
      "id": "Ld7j4M6vq7Nf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(topics)"
      ],
      "metadata": {
        "id": "e4Pg2PgjJ3AG"
      },
      "id": "e4Pg2PgjJ3AG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "topic coherence metrics"
      ],
      "metadata": {
        "id": "ZlWjnNpOnTCN"
      },
      "id": "ZlWjnNpOnTCN"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora.dictionary import Dictionary"
      ],
      "metadata": {
        "id": "s2SdFuwhnSMD"
      },
      "id": "s2SdFuwhnSMD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadfiletest():\n",
        "    pd_use = pd.read_csv(\"/content/drive/MyDrive/fresh_food.csv\",index_col=None)\n",
        "    \n",
        "    pos = pd_use[pd_use.label==1]\n",
        "    neg = pd_use[pd_use.label==0]\n",
        "    pos2=pos.sample( frac=0.2)\n",
        "    neg2=neg.sample( frac=0.2)\n",
        "    #print(pos)\n",
        "    \n",
        "    #neg = pd.read_csv('./data/neg.csv', header=None, index_col=None)\n",
        "    #pos = pd.read_csv('./data/pos.csv', header=None, index_col=None)\n",
        "\n",
        "    combined = np.concatenate((pos2[\"review\"], neg2[\"review\"]))\n",
        "    #combined = pd_use[\"review\"]\n",
        "   \n",
        "    y = np.concatenate((np.ones(len(pos), dtype=int), np.zeros(len(neg), dtype=int)))\n",
        "    \n",
        "\n",
        "    return combined, y\n"
      ],
      "metadata": {
        "id": "WZinItUkGgvx"
      },
      "id": "WZinItUkGgvx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('load test data')\n",
        "combined, y = loadfiletest()\n",
        "print(len(combined))\n",
        "print('data preparation')\n",
        "texts= tokenizer(combined)\n",
        "#texts= cut_words(combined)"
      ],
      "metadata": {
        "id": "kq2f6LMF-Nxr"
      },
      "id": "kq2f6LMF-Nxr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2id = Dictionary( texts )"
      ],
      "metadata": {
        "id": "OugNMnwbL7zz"
      },
      "id": "OugNMnwbL7zz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Coherence model\n",
        "#topics-attributes\n",
        "\n",
        "cm1 = CoherenceModel(topics=topics, \n",
        "                    texts=texts,\n",
        "                    coherence='c_v',  \n",
        "                    dictionary=word2id)\n",
        "\n",
        "cm2 = CoherenceModel(topics=topics, \n",
        "                    texts=texts,\n",
        "                    coherence='u_mass',  \n",
        "                    dictionary=word2id)\n",
        "\n",
        "coherence1 = cm1.get_coherence()\n",
        "\n",
        "coherence2 = cm2.get_coherence()\n"
      ],
      "metadata": {
        "id": "QkcbdP8t9tMs"
      },
      "id": "QkcbdP8t9tMs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(coherence1)\n",
        "print(coherence2)"
      ],
      "metadata": {
        "id": "uOqohdG2PRmm"
      },
      "id": "uOqohdG2PRmm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}